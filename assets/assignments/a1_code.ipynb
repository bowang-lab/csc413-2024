{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IZXk72LPCzNN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1: Implementing back propogation\n",
        "\n"
      ],
      "metadata": {
        "id": "g0JSJQ1FT70q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 1.0\n",
        "\n",
        "### **âœï¸ To Edit**\n",
        "\n",
        "Navigate to the top of the page and select `File > Download` or `File > Save a copy in Drive` to edit.\n",
        "\n",
        "### To submit\n",
        "\n",
        "You can download the notebook using `File > Download > Download .ipynb`. Submit a completed copy of your notebook with the final graphs rendered on MarkUS along with your submission for the written questions.\n",
        "\n",
        "### Overview\n",
        "\n",
        "\n",
        "In the programming portion of this assignment we will be implementing a automatic differentiation framework from scratch. This will be broken into two steps:\n",
        "\n",
        "* In part 1, you will implement a simple logistic regression and a gradient descent (GD) step.\n",
        "\n",
        "* In part 2, you will extend your implementation to a mutli-layered perceptron and fit it using the same gradient implementation.\n",
        "\n",
        "All the places where you need to implement something are marked with `# YOU'RE CODE HERE`."
      ],
      "metadata": {
        "id": "r072kcuB_XPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Datasets"
      ],
      "metadata": {
        "id": "5C3mblJAUcGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this homework we will work with two simple datasets of two 2d points. The classes are represented using binary targets that are either 0 or 1.\n",
        "\n",
        "Thus the features have a shape of `dataset_size x 2` and our targets have a shape of\n",
        "`dataset_size x 1`.\n",
        "\n",
        "The moons dataset which consists of two interlocking crescents. And the circles dataset which consists of separating an outer ring from an inner circle."
      ],
      "metadata": {
        "id": "xSKpQVFg-1Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data Generation Functions\n",
        "def generate_moons(n_samples=1000, noise=0.1):\n",
        "    X, y = make_moons(n_samples=n_samples, noise=noise, random_state=42)\n",
        "    return X.reshape(-1, 2), y.reshape(-1, 1)\n",
        "\n",
        "def generate_circles(n_samples=1000, noise=0.1, factor=0.3):\n",
        "    X, y = make_circles(n_samples=n_samples, noise=noise, factor=factor, random_state=42)\n",
        "    return X.reshape(-1, 2), y.reshape(-1, 1)\n",
        "\n",
        "# Generate datasets\n",
        "features_moons, targets_moons = generate_moons()\n",
        "features_circles, targets_circles = generate_circles()\n",
        "\n",
        "# Split data into train and test sets\n",
        "features_moons_train, features_moons_test, targets_moons_train, targets_moons_test = \\\n",
        "    train_test_split(features_moons, targets_moons, test_size=0.2, random_state=42)\n",
        "\n",
        "features_circles_train, features_circles_test, targets_circles_train, targets_circles_test = \\\n",
        "    train_test_split(features_circles, targets_circles, test_size=0.2, random_state=42)\n",
        "\n",
        "# Visualize the datasets\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(121)\n",
        "plt.scatter(features_moons[:, 0], features_moons[:, 1], c=targets_moons, cmap='viridis')\n",
        "plt.title('Moons Dataset')\n",
        "plt.subplot(122)\n",
        "plt.scatter(features_circles[:, 0], features_circles[:, 1], c=targets_circles, cmap='viridis')\n",
        "plt.title('Circles Dataset')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DxU_ABZyUVH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Module\n",
        "\n",
        "In class, we learned about the concept of a computational graph made up of variables connected by operations. In this assignment we will put that into practice by implementing our own simple neural network library. Similar to existing libraries like [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/Module), [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) and [Flax](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/module.html) we will build our library out of a series of neural network modules. Modules are the building blocks of neural network, like as a layer, activation functions, or normalization. In all of these frameworks modules are built to be composable, allowing complex networks to be built from simpler components.\n",
        "\n",
        "However, unlike the above frameworks we will **not** be building on top of an automatic differentiation engine and will be implementing the backward pass ourselves. Thus, in our case a module will encapsulate a parametric function and an implementation of that function's vector-Jacobian product (vJp). To achieve this we will use a simple `Module` class that builds directly on top of NumPy.\n",
        "\n",
        "Each module must implement two methods:\n",
        "\n",
        "* `forward` which computes the function $y = f_\\theta(x; t)$, and\n",
        "* `backward` which computes the functions vector-Jacobian products $v\\frac{\\partial y}{\\partial x}$ and $v\\frac{\\partial y}{âˆ‚ Î¸}$.\n",
        "\n",
        "Note that in our case, we treat the partial derivatives with respect to the parameters and the input separately.\n",
        "* The vJps with for the inputs should be returned as a tuple, one entry for each argument.\n",
        "* The vJps for the parameters should be *added* to their respective entries in the `_grad_parameters` dictionary.\n",
        "\n",
        "When invoked using `__call__` or `()`, the module will run the forward pass and storing intermediate results in memory for efficiently computing the vJp during the backward pass. This includes the inputs $x$, outputs $y$ of the function, and any side inputs $t$.\n",
        "\n",
        "We often don't want to compute the gradient with respect to all the arguments to our function $f$ (e.g. $t$). Therefore, we will maintain the convention we only compute the derivative with respect to positional arguments. Python allows us to enforce which arguments must be passed positionally and which must be given using keywords by adding `/ , *` as a delimiter in the function signature.\n",
        "\n",
        "Finally, for simplicity we will only allow our modules to return a single tensor as an output."
      ],
      "metadata": {
        "id": "UC08BtJoUio4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Module:\n",
        "    \"\"\"A component of a neural network that we can take the derivative with respect to.\"\"\"\n",
        "    _parameters: dict[str, NDArray]\n",
        "    \"\"\"Parameters of the module.\"\"\"\n",
        "    _parameter_grads: dict[str, NDArray]\n",
        "    \"\"\"Gradients of the parameters.\"\"\"\n",
        "\n",
        "    _inputs: tuple[NDArray, ...] | None\n",
        "    \"\"\"The stashed inputs to the module from the most recent forward pass.\"\"\"\n",
        "    _input_kwargs: dict[str, NDArray] | None\n",
        "    \"\"\"The stahed side arguments to the module from the most recent pass.\"\"\"\n",
        "    _output: NDArray | None\n",
        "    \"\"\"The stashed output of the module from the most recent forward pass.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._parameters = {}\n",
        "        self._parameter_grads = {}\n",
        "        self._inputs = None\n",
        "        self._input_kwargs = None\n",
        "        self._output = None\n",
        "\n",
        "    def forward(self, *args: list[NDArray], **kwargs) -> NDArray:\n",
        "        \"\"\"\n",
        "        Compute the forward pass of the module and return the output.\n",
        "\n",
        "        If derivatives are required this method should *not* be called directly. Instead,\n",
        "        you should invoke it using module(*args, **kwargs).\n",
        "\n",
        "        Args:\n",
        "            *args: List of input NDArrays we want to take a derivative with respect to\n",
        "            **kwargs: Additional keyword arguments we will not produce gradients for\n",
        "\n",
        "        Returns:\n",
        "            NDArray: Output of the forward pass\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    def backward(self, grad_output: NDArray) -> tuple[NDArray, ...]:\n",
        "        \"\"\"\n",
        "        Accumulate gradients for the parameters of the module and return the input gradients.\n",
        "\n",
        "        Args:\n",
        "            grad_output (NDArray): Gradient of the loss with respect to the module's output\n",
        "\n",
        "        Returns:\n",
        "            tuple[NDArray, ...]: Gradients with respect to the module's inputs\n",
        "        \"\"\"\n",
        "        ...\n",
        "\n",
        "    def get_parameters(self):\n",
        "        \"\"\"Return the parameters of the module and all its submodules.\"\"\"\n",
        "        return self._parameters\n",
        "\n",
        "    def get_parameter_grads(self):\n",
        "        \"\"\"Return the gradients of the parameters of the module and all its submodules.\"\"\"\n",
        "        return self._parameter_grads\n",
        "\n",
        "    def __call__(self, *args: NDArray, **kwargs) -> NDArray:\n",
        "        \"\"\"Run the forward pass of the module and populate the computation graph.\"\"\"\n",
        "        self._inputs = args\n",
        "        self._input_kwargs = kwargs\n",
        "        self._output = self.forward(*args, **kwargs)\n",
        "        return self._output\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"Reset all the gradents with respect to the parameters to zero.\"\"\"\n",
        "        self._parameter_grads = {\n",
        "            name: np.zeros_like(value) for name, value in self._parameters.items()\n",
        "        }\n",
        "\n",
        "\n",
        "def merge_dicts(**kwargs: dict[str, NDArray]) -> dict[str, NDArray]:\n",
        "    \"\"\"A helper function for merging dictionaries of numpy arrays.\"\"\"\n",
        "    return {kw + '.' + k: v for kw, d in kwargs.items() for k, v in d.items()}"
      ],
      "metadata": {
        "id": "tXdyoAoMUIAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing vector-jacobian product implementations"
      ],
      "metadata": {
        "id": "whoPyL0Xaguq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment we will provide you with a handy utility for testing your module implementations: `test_vjp_inputs` and `test_vjp_parameters`.\n",
        "\n",
        "These functions compare the results of computing the jacobain of the model using a sqeuence of numerical gradients $\\frac{\\partial y}{\\partial x}\\mathbb u\\approx\\frac{f(\\mathbb x + h\\mathbb u) - f(\\mathbb x - h\\mathbb u)}{2h}$ for small $h$. To the results of computing the jacobian using your custom vJP implementations."
      ],
      "metadata": {
        "id": "KQF2Y2Qpal_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "\n",
        "def compute_jacobians(\n",
        "    func: Callable[[], np.ndarray],\n",
        "    x: np.ndarray,\n",
        "    backward_func: Callable[[np.ndarray], np.ndarray],\n",
        "    epsilon: float = 1e-6\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Compute both analytical and numerical Jacobians for a given function with respect to a single input.\n",
        "\n",
        "    Args:\n",
        "        func: The forward function that returns the output.\n",
        "        x: The input array to compute Jacobians with respect to.\n",
        "        backward_func: The backward function to compute analytical Jacobians.\n",
        "        epsilon: The step size for finite differences (default: 1e-6).\n",
        "\n",
        "    Returns:\n",
        "        A tuple of (analytical_jac, numerical_jac).\n",
        "    \"\"\"\n",
        "    output = func()\n",
        "    analytical_jac = np.zeros(output.shape + x.shape)\n",
        "    numerical_jac = np.zeros(output.shape + x.shape)\n",
        "\n",
        "    # Compute analytical Jacobian\n",
        "    for out_idx in np.ndindex(output.shape):\n",
        "        output_grad = np.zeros_like(output)\n",
        "        output_grad[out_idx] = 1\n",
        "        grad = backward_func(output_grad)\n",
        "        analytical_jac[out_idx + (...,)] = grad\n",
        "\n",
        "    # Compute numerical Jacobian\n",
        "    for in_idx in np.ndindex(x.shape):\n",
        "        original_value = x[in_idx].copy()\n",
        "        x[in_idx] = original_value + epsilon\n",
        "        output_plus = func()\n",
        "        x[in_idx] = original_value - epsilon\n",
        "        output_minus = func()\n",
        "        x[in_idx] = original_value  # Restore original value\n",
        "        numerical_jac[(...,) + in_idx] = (output_plus - output_minus) / (2 * epsilon)\n",
        "\n",
        "    return analytical_jac, numerical_jac\n",
        "\n",
        "\n",
        "def test_vjp_inputs(module: Module, *inputs: NDArray, **kwargs) -> None:\n",
        "    def forward():\n",
        "        return module(*inputs, **kwargs)\n",
        "\n",
        "    def backward(grad_output):\n",
        "        return module.backward(grad_output)\n",
        "\n",
        "    for i, input_array in enumerate(inputs):\n",
        "        analytical_jac, numerical_jac = compute_jacobians(\n",
        "            func=forward,\n",
        "            x=input_array,\n",
        "            backward_func=lambda g: backward(g)[i]\n",
        "        )\n",
        "\n",
        "        np.testing.assert_allclose(\n",
        "            analytical_jac, numerical_jac,\n",
        "            rtol=1e-3, atol=1e-5,\n",
        "            err_msg=f'Gradient check failed for input {i}'\n",
        "        )\n",
        "\n",
        "    print(\"All input gradients passed the test.\")\n",
        "\n",
        "def test_vjp_parameters(module: Module, *inputs: NDArray, **kwargs) -> None:\n",
        "    parameters = module.get_parameters()\n",
        "\n",
        "    def forward():\n",
        "        return module(*inputs, **kwargs)\n",
        "\n",
        "    for param_name, param_value in parameters.items():\n",
        "        def backward(grad_output):\n",
        "            module.zero_grad()\n",
        "            module.backward(grad_output)\n",
        "            return module.get_parameter_grads()[param_name]\n",
        "\n",
        "        analytical_jac, numerical_jac = compute_jacobians(\n",
        "            func=forward,\n",
        "            x=param_value,\n",
        "            backward_func=backward\n",
        "        )\n",
        "\n",
        "        np.testing.assert_allclose(\n",
        "            analytical_jac, numerical_jac,\n",
        "            rtol=1e-3, atol=1e-5,\n",
        "            err_msg=f'Gradient check failed for parameter {param_name}'\n",
        "        )\n",
        "\n",
        "    print(\"All parameter gradients passed the test.\")"
      ],
      "metadata": {
        "id": "6MURI3UCBcXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An example module\n",
        "\n",
        "For reference here is a trival operation that computes an operation:\n",
        "\n",
        "$\n",
        "    f_\\alpha(a, b; s) = \\cases{\\alpha(a + b) & s = false \\\\ \\alpha (a - b)  & s = true}\n",
        "$\n"
      ],
      "metadata": {
        "id": "YBYE7YCYkxe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddOrSubtractAndMultiply(Module):\n",
        "    def __init__(self, alpha: float = 0.1):\n",
        "        super().__init__()\n",
        "        self._parameters['alpha'] = np.array(alpha)\n",
        "        self.zero_grad()\n",
        "\n",
        "    def forward(self, a: NDArray, b: NDArray, /, *, subtract: bool = False) -> NDArray:\n",
        "        \"\"\"\n",
        "        Add or subtract the two batches of scalers and then multiply by a scalar.\n",
        "\n",
        "        Args:\n",
        "            a: A batch of scalers with shape (batch_size, 1)\n",
        "            b: A batch of scalers with shape (batch_size, 1)\n",
        "\n",
        "        KWArgs:\n",
        "            subtract (bool, optional): Whether to subtract the tensors. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            NDArray: Result of the operation\n",
        "        \"\"\"\n",
        "        value = self._parameters['alpha']\n",
        "        return (a + b) * value if not subtract else (a - b) * value\n",
        "\n",
        "    def backward(self, grad_output: NDArray) -> tuple[NDArray, NDArray]:\n",
        "        \"\"\" Compute the backward pass for this module.\n",
        "\n",
        "        Args:\n",
        "            grad_output: (batch_size, 1)\n",
        "\n",
        "        Returns:\n",
        "            Gradient of the loss with respect to each of the of this module\n",
        "        \"\"\"\n",
        "        value = self._parameters['alpha']\n",
        "        subtract = self._input_kwargs['subtract']\n",
        "        output = self._output\n",
        "\n",
        "        grad_a = value * grad_output\n",
        "        grad_b = value * grad_output if not subtract else -value * grad_output\n",
        "        self._parameter_grads['alpha'] += (grad_output * self._output / value).sum()\n",
        "\n",
        "        return (grad_a, grad_b)\n",
        "\n",
        "a = np.random.randn(10, 1)\n",
        "b = np.random.randn(10, 1)\n",
        "add_or_subtract_and_multiply = AddOrSubtractAndMultiply()\n",
        "\n",
        "# Example usage\n",
        "c = add_or_subtract_and_multiply(a, b, subtract=False)\n",
        "\n",
        "test_vjp_inputs(\n",
        "    add_or_subtract_and_multiply,\n",
        "    a,\n",
        "    b,\n",
        "    subtract=False\n",
        ")\n",
        "\n",
        "test_vjp_parameters(\n",
        "    add_or_subtract_and_multiply,\n",
        "    a,\n",
        "    b,\n",
        "    subtract=False\n",
        ")"
      ],
      "metadata": {
        "id": "UxmxZqxphmGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Logistic Regression with Full Batch GD [3.5 pts]"
      ],
      "metadata": {
        "id": "GaD_Tdi8CpUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a warm-up we implement a basic version of logistic regression with a cross entropy loss. Logistic regression with this objective does not have a closed form solution. Here we will simply apply full batch gradient decent to find a close to optimal solution.\n",
        "\n",
        "\n",
        "$$\n",
        "    \\mathcal{J}(\\mathbf w, \\mathbf b) = \\frac{1}{N}\\sum_{i=0}^{N} \\mathcal{L}_\\theta (x_i, t_i)\n",
        "$$\n",
        "\n",
        "$$\n",
        "    \\mathcal{L}_{BCE} = -[t\\log(Ïƒ(\\mathbf z)) + (1 - t)\\log(1 - \\sigma(\\mathbf z))]\n",
        "$$\n",
        "\n",
        "$$\n",
        "    \\mathbf z = \\mathbf w^\\top x + \\mathbf b\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NU8mT0Eja7A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1.1 Implement the binary cross entropy loss [1 pts]\n",
        "\n",
        "Implement the `forward` and `backard` pass for the binary cross entropy loss. Each funciton is worth 0.5 points.\n",
        "\n",
        "Note that this module implements both the cross enropy loss and averaging over the dataset. Thus it should return a scalar."
      ],
      "metadata": {
        "id": "L4UZHjqJpZWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryCrossEntropy(Module):\n",
        "    \"\"\"Binary cross entropy loss\"\"\"\n",
        "    def forward(self, logits: NDArray, /, *, targets: NDArray) -> NDArray:\n",
        "        \"\"\"\n",
        "        Compute the binary cross entropy loss.\n",
        "\n",
        "        Args:\n",
        "            logits: Predicted logits, shape (batch_size, 1)\n",
        "            targets: True binary labels, shape (batch_size, 1)\n",
        "\n",
        "        Returns:\n",
        "            Average binary cross entropy loss across the batch with should be a scaler.\n",
        "        \"\"\"\n",
        "        assert logits.shape == targets.shape, f\"Shapes mismatch: logits {logits.shape}, targets {targets.shape}\"\n",
        "        assert logits.ndim == 2 and logits.shape[1] == 1, f\"Expected logits shape (batch_size, 1), got {logits.shape}\"\n",
        "\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "\n",
        "    def backward(self, grad_output: NDArray) -> tuple[NDArray]:\n",
        "        \"\"\"\n",
        "        Compute the gradient of the binary cross entropy loss.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient of the loss with respect to the output, scalar\n",
        "\n",
        "        Returns:\n",
        "            Gradient with respect to logits, shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        assert grad_output.shape == (), f\"Expected scalar grad_output, got shape {grad_output.shape}\"\n",
        "\n",
        "        # YOU'RE CODE HERE\n",
        "        return (logits_grad,)\n",
        "\n",
        "test_vjp_inputs(\n",
        "    BinaryCrossEntropy(),\n",
        "    np.random.randn(10, 1),\n",
        "    targets=np.random.randint(0, 2, 10)[:, None]\n",
        ")"
      ],
      "metadata": {
        "id": "TuB35PdFCvme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1.2: Implement a general linear layer [1 pts]\n",
        "\n",
        "One of the most common layers types in all of deep learning is a \"linear\" layer. This layer will serve as a component in both our logistic regression and later in our MLP. These are typically actually *affine* (not linear) transformations since they include a bias term:  \n",
        "\n",
        "$$f_{\\mathbf{W}, {\\mathbf{b}}}(x) = W\\mathbf x + \\mathbf b$$\n",
        "\n",
        "where the parameter $W \\in \\mathbb{R^{n\\times m}}$ and $\\mathbb b \\in \\mathbb R^n$."
      ],
      "metadata": {
        "id": "IZXk72LPCzNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"A parameterized affine transformation\n",
        "\n",
        "    Parameters:\n",
        "        weights: Weight matrix, shape (input_size, output_size)\n",
        "        bias: Bias vector, shape (output_size,)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self._parameters['weights'] = np.random.randn(input_size, output_size) * np.sqrt(2. / input_size)\n",
        "        self._parameters['bias'] = np.zeros((output_size,))\n",
        "        self.zero_grad()\n",
        "\n",
        "    def forward(self, features: NDArray) -> NDArray:\n",
        "        \"\"\"\n",
        "        Forward pass of the linear layer.\n",
        "\n",
        "        Args:\n",
        "            features: Input features, shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            Output of the linear layer, shape (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        assert features.ndim == 2, f\"Expected 2D input, got shape {features.shape}\"\n",
        "        assert features.shape[1] == self._parameters['weights'].shape[0], f\"Input size mismatch: {features.shape[1]} != {self._parameters['weights'].shape[0]}\"\n",
        "\n",
        "        # Begin Part of solution\n",
        "        return features @ self._parameters['weights'] + self._parameters['bias'][None, :]\n",
        "        # End Part of solution\n",
        "\n",
        "    def backward(self, grad_output: NDArray) -> tuple[NDArray]:\n",
        "        \"\"\"\n",
        "        Backward pass of the linear layer.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient of the loss with respect to the output, shape (batch_size, output_size)\n",
        "\n",
        "        Returns:\n",
        "            Gradient with respect to the input features, shape (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        assert grad_output.ndim == 2, f\"Expected 2D grad_output, got shape {grad_output.shape}\"\n",
        "        assert grad_output.shape[1] == self._parameters['weights'].shape[1], f\"Gradient output size mismatch: {grad_output.shape[1]} != {self._parameters['weights'].shape[1]}\"\n",
        "\n",
        "        # Begin Part of solution\n",
        "        features = self._inputs[0]\n",
        "        grad_weights = np.dot(features.T, grad_output)\n",
        "        grad_bias = np.sum(grad_output, axis=0)\n",
        "\n",
        "        self._parameter_grads['weights'] += grad_weights\n",
        "        self._parameter_grads['bias'] += grad_bias\n",
        "\n",
        "        grad_input = np.dot(grad_output, self._parameters['weights'].T)\n",
        "        # End Part of solution\n",
        "        return (grad_input,)\n",
        "\n",
        "linear = Linear(10, 5)\n",
        "features = np.random.randn(3, 10)\n",
        "test_vjp_inputs(linear, features)\n",
        "test_vjp_parameters(linear, features)"
      ],
      "metadata": {
        "id": "ikcaOgULCzpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1.3: Implement the binary logistic regression [0.5 pts]\n",
        "\n",
        "Chain together your linear layer and cross entropy layer to form a complete logistic regression module. Note that the forward pass should return a scalar **loss**."
      ],
      "metadata": {
        "id": "0VYXFq7BC-yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryLogisticRegression(Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.linear = Linear(input_size, 1)\n",
        "        self.loss = BinaryCrossEntropy()\n",
        "        self.zero_grad()\n",
        "\n",
        "    def forward(self, features: NDArray, /, *, targets: NDArray) -> NDArray:\n",
        "        \"\"\"\n",
        "        Forward pass of the logistic regression model.\n",
        "\n",
        "        Args:\n",
        "            features: Input features, shape (batch_size, input_size)\n",
        "            targets: True binary labels, shape (batch_size, 1)\n",
        "\n",
        "        Returns:\n",
        "            NDArray: Scalar loss value\n",
        "        \"\"\"\n",
        "        assert features.ndim == 2, f\"Expected 2D features, got shape {features.shape}\"\n",
        "        assert targets.ndim == 2 and targets.shape[1] == 1, f\"Expected targets shape (batch_size, 1), got {targets.shape}\"\n",
        "        assert features.shape[0] == targets.shape[0], f\"Batch size mismatch: features {features.shape[0]}, targets {targets.shape[0]}\"\n",
        "\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "    def backward(self, grad_output: NDArray) -> tuple[NDArray]:\n",
        "        \"\"\"\n",
        "        Backward pass of the logistic regression model.\n",
        "\n",
        "        Args:\n",
        "            grad_output (NDArray): Gradient of the loss with respect to the output, scalar\n",
        "\n",
        "        Returns:\n",
        "            tuple[NDArray]: Gradient with respect to the input features, shape (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        assert grad_output.shape == (), f\"Expected scalar grad_output, got shape {grad_output.shape}\"\n",
        "\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "    def predict(self, features: NDArray) -> NDArray:\n",
        "        \"\"\"\n",
        "        Predict binary labels using the logistic regression model. # TODO (lev) improve this docstring\n",
        "\n",
        "        Args:\n",
        "            features (NDArray): Input features, shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            NDArray: Predicted binary labels, shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        logits = self.linear(features)\n",
        "        sigmoid = 1 / (1 + np.exp(-logits))\n",
        "        return (sigmoid >= 0.5).astype(int)\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.linear.get_parameters()\n",
        "\n",
        "    def get_parameter_grads(self):\n",
        "        return self.linear.get_parameter_grads()\n",
        "\n",
        "test_vjp_inputs(\n",
        "    BinaryLogisticRegression(10),\n",
        "    np.random.randn(3, 10),\n",
        "    targets=np.random.randint(0, 2, 3)[:, None]\n",
        ")"
      ],
      "metadata": {
        "id": "2x2B1cnHC-Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1.4: Full batch Gradient Descent [1 pts]"
      ],
      "metadata": {
        "id": "6Wa6e64BDCWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a single step of gradient decent on our classifier with respect to the entire dataset.\n",
        "\n",
        "> âš \n",
        ">\n",
        "> We will reuse this function for learning our MLP. Thus you should make it as general as possible!\n",
        "\n",
        "> ğŸ’¡ Hint\n",
        ">\n",
        "> Use `get_parameter_grads` and `get_parameters` to make your gradent decent implementation agnostic to how the classifier is implemented.\n"
      ],
      "metadata": {
        "id": "9BWTNbw1FW5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gd_step(classifier: Module, input_features: NDArray, targets: NDArray, learning_rate: float) -> float:\n",
        "    \"\"\"Perform a single step of gradient descent on the parameters of the classifier.\n",
        "\n",
        "        Args:\n",
        "            classifier: The module we will take a step on. It should accept\n",
        "                input_features and targets as arguments. It will be updated in-place.\n",
        "            input_features: A (batch_size, input_size) array of input features\n",
        "            targets: A (batch_size, 1) array of integer targets\n",
        "            learning_rate: The learning rate to use for SGD.\n",
        "\n",
        "        Returns:\n",
        "            The loss of the model on the batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # YOU'RE CODE HERE\n",
        "    return loss"
      ],
      "metadata": {
        "id": "l1xsdux6rQFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1.5 Run the training and visualize the results [0 pts]\n",
        "\n",
        "Observe, how the classification accuracy changes with training. Why can't we get a training accurcay of above about 90% on the moons dataset?"
      ],
      "metadata": {
        "id": "d-DLJSrjtdEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def compute_accuracy(model, features, targets):\n",
        "    predictions = model.predict(features)\n",
        "    return np.mean(predictions == targets)\n",
        "\n",
        "def fit_logistic_regression(\n",
        "        model: Module,\n",
        "        train_features: NDArray,\n",
        "        train_targets: NDArray,\n",
        "        val_features: NDArray,\n",
        "        val_targets: NDArray,\n",
        "        num_iterations: int = 1000,\n",
        "        learning_rate: float = 0.01,\n",
        "        eval_every: int = 10\n",
        "    ):\n",
        "    steps = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    tb = tqdm(range(num_iterations), desc='Training', leave=False)\n",
        "\n",
        "    for i in tb:\n",
        "        loss = gd_step(model, train_features, train_targets, learning_rate)\n",
        "\n",
        "        if i % eval_every == 0:\n",
        "            steps.append(i)\n",
        "            train_losses.append(loss)\n",
        "            train_acc = compute_accuracy(model, train_features, train_targets)\n",
        "            val_acc = compute_accuracy(model, val_features, val_targets)\n",
        "            train_accuracies.append(train_acc)\n",
        "            val_accuracies.append(val_acc)\n",
        "            tb.set_postfix(train_loss=loss, train_acc=train_acc, val_acc=val_acc)\n",
        "\n",
        "    return steps, train_losses, train_accuracies, val_accuracies\n",
        "\n",
        "def plot_decision_boundary(model, X, y, ax=None):\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02),\n",
        "                           np.arange(x2_min, x2_max, 0.02))\n",
        "    Z = model.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "\n",
        "    cmap = ListedColormap(['#FFAAAA', '#AAAAFF'])\n",
        "    ax.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, edgecolor='black')\n",
        "    ax.set_xlim(xx1.min(), xx1.max())\n",
        "    ax.set_ylim(xx2.min(), xx2.max())\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "    return ax\n",
        "\n",
        "def plot_results(\n",
        "        *,\n",
        "        steps,\n",
        "        train_losses,\n",
        "        train_accuracies,\n",
        "        val_accuracies,\n",
        "        model,\n",
        "        X,\n",
        "        y,\n",
        "    ):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 18))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(steps, train_losses)\n",
        "    ax1.set_xlabel('Iterations')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training Loss over Iterations')\n",
        "\n",
        "    # Plot accuracies\n",
        "    ax2.plot(steps, train_accuracies, label='Train Accuracy')\n",
        "    ax2.plot(steps, val_accuracies, label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Iterations')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Train and Validation Accuracy over Iterations')\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3 = plot_decision_boundary(model, X, y, ax=ax3)\n",
        "    ax3.set_title('Final Decision Boundary')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "W8hMaKUFDV4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data\n",
        "targets_moons_train = targets_moons_train.reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
        "targets_moons_test = targets_moons_test.reshape(-1, 1)  # Reshape to (n_samples, 1)\n",
        "\n",
        "# Initialize and train the model\n",
        "input_size = features_moons_train.shape[1]\n",
        "model = BinaryLogisticRegression(input_size)\n",
        "steps, train_losses, train_accuracies, val_accuracies = fit_logistic_regression(\n",
        "    model,\n",
        "    features_moons_train,\n",
        "    targets_moons_train,\n",
        "    features_moons_test,\n",
        "    targets_moons_test,\n",
        "    num_iterations=1000,\n",
        "    learning_rate=0.0001,\n",
        "    eval_every=10\n",
        ")\n",
        "\n",
        "# Plot the training curves\n",
        "plot_results(\n",
        "    steps=steps,\n",
        "    train_losses=train_losses,\n",
        "    train_accuracies=train_accuracies,\n",
        "    val_accuracies=val_accuracies,\n",
        "    model=model,\n",
        "    X=features_moons,\n",
        "    y=targets_moons\n",
        ")"
      ],
      "metadata": {
        "id": "zdc6b5qEtuT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Multi-Layer Perceptron (MLP) [2.5 pts]\n",
        "\n",
        "A Multi-Layer Perceptron (MLP) for binary classification extends logistic regression by adding hidden layers allowing the model to learn a more expressive feature basis and fit non-linearly separable data.\n",
        "\n",
        "Forward Pass:\n",
        "* $\\mathbf{a}^{[0]} = \\mathbf x$\n",
        "* For each layer $l$ from $1$ to $L$:\n",
        "\t* $\\mathbf{z}^{[l]} = W^{[l]}\\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}$\n",
        "\t* $\\mathbf{a}^{[l]} = g(\\mathbf{z}^{[l]})$\n",
        "\t* $y = W^{O}\\mathbf{a}^{[l]} + \\mathbf{b}^{O}$\n",
        "* $\\mathcal{L}_{BCE} = -[t\\log(\\sigma(y)) + (1 - t)\\log(1 - \\sigma(y))]$\n",
        "\n",
        "Here $g$ is our activation function. For this assignment we will consider 2 activation functions $tanh(x)$ and $ReLU(x) = max(0, x)$."
      ],
      "metadata": {
        "id": "fR_kMk0FDoOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.1: Activation Functions [0.5 pts]"
      ],
      "metadata": {
        "id": "BNATXHwTDr7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(Module):\n",
        "    def forward(self, input: NDArray):\n",
        "        \"\"\"Run tanh activation function elementwise.\n",
        "\n",
        "        Args:\n",
        "            input: Input data, shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            Output of the tanh activation function, shape (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "    def backward(self, grad_output: NDArray):\n",
        "        \"\"\"Compute the gradient of the tanh activation function.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient of the loss with respect to the output, shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            Gradient of the loss with respect to the input, shape (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "test_vjp_inputs(Tanh(), np.random.randn(10, 1))"
      ],
      "metadata": {
        "id": "xGdMqiNcDtRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(Module):\n",
        "    \"\"\"ReLU activation function\"\"\"\n",
        "    def forward(self, input):\n",
        "        \"\"\"Run ReLU activation function elementwise.\n",
        "\n",
        "        Args:\n",
        "            input: Input data, shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            Output of the ReLU activation function, shape (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"Compute the gradient of the ReLU activation function.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient of the loss with respect to the output, shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            Gradient of the loss with respect to the input, shape (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "test_vjp_inputs(ReLU(), np.random.randn(10, 1))"
      ],
      "metadata": {
        "id": "bmHS9CxNDyU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.2: The Sequential module [1 pts]\n",
        "\n",
        "\n",
        "Many neural networks consist of a sequence of modules with a single output and input that are chained together. Unlike the modules we have worked with to this point the `Sequential` module represents a more abstract operation. Like its [pytorch equvilent](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html), it represents composing together a sequence of modules and feeding the output of one into the input of the next in order.\n",
        "\n"
      ],
      "metadata": {
        "id": "dbupnHQUD8_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"Run a sequence of layers feeding the outputs of one into another.\"\"\"\n",
        "\n",
        "    layers: tuple[Module]\n",
        "    \"\"\"Layers in the sequence.\"\"\"\n",
        "\n",
        "    def __init__(self, *layers: Module):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.zero_grad()\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Run the sequence of layers on the input.\n",
        "\n",
        "        Args:\n",
        "            input: Input data, shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            Output of the last layer, shape (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"Compute the gradient of the loss with respect to the input.\n",
        "\n",
        "        Args:\n",
        "            grad_output: Gradient of the loss with respect to the output, shape (batch_size, output_size)\n",
        "\n",
        "        Returns:\n",
        "            Gradient of the loss with respect to the input, shape (batch_size, input_size)\n",
        "        \"\"\"\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return merge_dicts(\n",
        "            **{f'{i}': layer.get_parameters()\n",
        "                for i, layer in enumerate(self.layers)\n",
        "                if isinstance(layer, Module)}\n",
        "        )\n",
        "\n",
        "    def get_parameter_grads(self):\n",
        "        return merge_dicts(\n",
        "            **{f'{i}': layer.get_parameter_grads()\n",
        "               for i, layer in enumerate(self.layers)\n",
        "               if isinstance(layer, Module)}\n",
        "        )\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Module):\n",
        "                layer.zero_grad()\n",
        "\n",
        "input_size = 10\n",
        "hidden_size = 5\n",
        "output_size = 3\n",
        "sequential = Sequential(\n",
        "    Linear(input_size, hidden_size),\n",
        "    ReLU(),\n",
        "    Linear(hidden_size, output_size)\n",
        ")\n",
        "\n",
        "\n",
        "test_vjp_inputs(sequential, np.random.randn(10, 10))\n",
        "test_vjp_parameters(sequential, np.random.randn(10, 10))"
      ],
      "metadata": {
        "id": "4fFYv6UaEC1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.3 The Binary Classification MLP [1 pts]\n",
        "\n",
        "Now combining everything we have done so far will implement a simple multilayer perceptorn with an arbitrary number of hidden layers and widths per layer.\n",
        "\n",
        "We will seperate our MLP into two parts the `self.net` and the `self.loss`. You're first task will be to fill the layers list with the apropriate moduels to implement the computation graph\n",
        "descibed above.\n",
        "\n",
        "Next you will implement the forward pass, and finally the backward pass."
      ],
      "metadata": {
        "id": "i0BWIjE8EE-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryClassificationMLP(Module):\n",
        "    \"\"\"\n",
        "    A multi-layer perceptron for binary classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_sizes: list[int], activation=ReLU):\n",
        "        \"\"\"Create the MLP.\n",
        "\n",
        "        Args:\n",
        "            input_size: The size of the input vector.\n",
        "            hidden_sizes: A list of the sizes of the hidden layers.\n",
        "            activation: The activation function to use.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        sizes = [input_size] + hidden_sizes + [1]\n",
        "        layers = []\n",
        "\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "        self.net = Sequential(*layers)\n",
        "        self.loss = BinaryCrossEntropy()\n",
        "\n",
        "    def forward(self, features: NDArray, targets: NDArray):\n",
        "        \"\"\"Compute the loss of the network on these features and targets.\n",
        "\n",
        "        Args:\n",
        "            features: A (batch_size, input_size) array of input features.\n",
        "            targets: A (batch_size, 1) array of binary targets.\n",
        "\n",
        "        Returns:\n",
        "            The loss of the network on these features and targets.\n",
        "        \"\"\"\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"Compute the gradient of the loss with respect to the parameters of the network.\n",
        "\n",
        "        Args:\n",
        "            grad_output: A (batch_size, 1) array of gradients of the loss with respect to the output of the network.\n",
        "\n",
        "        Returns:\n",
        "            A (batch_size, input_size) array of gradients of the loss with respect to the input features.\n",
        "        \"\"\"\n",
        "        # YOU'RE CODE HERE\n",
        "\n",
        "    def predict(self, features: NDArray) -> NDArray:\n",
        "        logits = self.net(features)\n",
        "        sigmoid = 1 / (1 + np.exp(-logits))\n",
        "        return (sigmoid >= 0.5).astype(int)\n",
        "\n",
        "    def get_parameters(self):\n",
        "        return self.net.get_parameters()\n",
        "\n",
        "    def get_parameter_grads(self):\n",
        "        return self.net.get_parameter_grads()\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.net.zero_grad()\n",
        "\n",
        "\n",
        "mlp = BinaryClassificationMLP(input_size=2, hidden_sizes=[10, 10], activation=ReLU)\n",
        "test_vjp_inputs(mlp, np.random.randn(10, 2), targets=np.random.randint(0, 2, 10)[:, None])\n",
        "test_vjp_parameters(mlp, np.random.randn(10, 2), targets=np.random.randint(0, 2, 10)[:, None])"
      ],
      "metadata": {
        "id": "8wE7gF9IXWoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2.4: Train your MLP [0 pts]\n",
        "\n",
        "Train you newly minted multi-layer perceptron. Try switching out the activation function and training on both datasets. What happens as you increase the depth of your network? What about changing its width? Do you notice anything when you change from $ReLU$ to $tanh$ activations?"
      ],
      "metadata": {
        "id": "NCBPm0D_Xa_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_mlp(\n",
        "        model,\n",
        "        X,\n",
        "        y,\n",
        "        X_val,\n",
        "        y_val,\n",
        "        num_iterations: int = 1000,\n",
        "        learning_rate: float = 0.01,\n",
        "        eval_every: int = 10\n",
        "    ):\n",
        "    steps = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    tb = tqdm(range(num_iterations), desc='Training', leave=False)\n",
        "\n",
        "    for i in tb:\n",
        "        loss = gd_step(model, X, y, learning_rate)\n",
        "\n",
        "        if i % eval_every == 0:\n",
        "            steps.append(i)\n",
        "            train_losses.append(loss)\n",
        "            train_acc = compute_accuracy(model, X, y)\n",
        "            val_acc = compute_accuracy(model, X_val, y_val)\n",
        "            train_accuracies.append(train_acc)\n",
        "            val_accuracies.append(val_acc)\n",
        "            tb.set_postfix(train_loss=loss, train_acc=train_acc, val_acc=val_acc)\n",
        "\n",
        "    return steps, train_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "id": "fDphRs3iXmVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate MLP with Sigmoid activation on moons dataset\n",
        "mlp = BinaryClassificationMLP(input_size=2, hidden_sizes=[10, 10], activation=ReLU)\n",
        "\n",
        "train_features = features_moons_train # features_circles_train\n",
        "train_targets = targets_moons_train # targets_circles_train\n",
        "val_features = features_moons_test # features_circles_test\n",
        "val_targets = targets_moons_test # targets_circles_test\n",
        "\n",
        "steps, losses, train_acc, val_acc = fit_mlp(\n",
        "    mlp, train_features, train_targets, val_features, val_targets,\n",
        "    num_iterations=1000, learning_rate=0.001\n",
        ")\n",
        "\n",
        "# Plot results for moons dataset\n",
        "plot_results(\n",
        "    steps=steps,\n",
        "    train_losses=losses,\n",
        "    train_accuracies=train_acc,\n",
        "    val_accuracies=val_acc,\n",
        "    model=mlp,\n",
        "    X=train_features,\n",
        "    y=train_targets\n",
        ")"
      ],
      "metadata": {
        "id": "wekaZO6DYWz6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}